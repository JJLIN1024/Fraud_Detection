# -*- coding: utf-8 -*-
"""Fraud_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nzhm5nPPmkg43SIFvgttMfRoYdhIlfDt

# **Fraud Detection**

## PreProcessing

### Get Data

#### Download Dataset
"""

f1id = '1W0EL3w6qxHRa5ZFPYhYl87Piv7Y94ELG'
f2id = '1mJv80_a17wXnXydZEPGXKFtnQRd5REsc'

!pip3 install gdown

import gdown

url = 'https://drive.google.com/uc?id=%s'%(f1id)

output = 'train.csv'
gdown.download(url, output, quiet=False)
url = 'https://drive.google.com/uc?id=%s'%(f2id)
output = 'test.csv'
gdown.download(url, output, quiet=False)

"""#### Import package"""

# Basic lib

import numpy as np 
import pandas as pd 

# Preprocess
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
!pip install autoviz datasist pandas_profiling 
from autoviz.AutoViz_Class import AutoViz_Class

from pandas_profiling import ProfileReport
import datasist as ds 
from sklearn.feature_selection import RFECV

# Classifier Libraries
from sklearn.linear_model import LogisticRegression

from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
import lightgbm as lgb

# Performance Evaluation tools
from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
from sklearn.model_selection import TimeSeriesSplit


# imblearn
from imblearn.pipeline import Pipeline, make_pipeline
from imblearn.under_sampling import NearMiss
from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
from imblearn.combine import SMOTEENN, SMOTETomek
from imblearn.metrics import classification_report_imbalanced

# Utils
import time
import gc
from collections import Counter
import warnings
warnings.filterwarnings("ignore")

"""#### Read Data"""

df_train = pd.read_csv('./train.csv')
df_test = pd.read_csv('./test.csv')
df_all = pd.concat((df_train, df_test), 0)

df_train.name = 'Training Set'
df_test.name = 'Test Set'
df_all.name = 'All Set' 

dfs = [df_train, df_test]

# Due to RAM limit on colab, need to reduce memory usage
def reduce_mem(df):
    start_mem_usg = df.memory_usage().sum() / (1024*1024)
    print("Memory usage of properties dataframe is :",start_mem_usg," MB")
    
    for col in df.keys():
        if df[col].dtype == int:
            Max = df[col].max()
            Min = df[col].min()
            if -128 < Min and Max < 127:
                df[col] = df[col].astype(np.int8)
            elif -32768 < Min and Max < 32767:
                df[col] = df[col].astype(np.int16)
            elif -2147483648 < Min and Max < 2147483647:
                df[col] = df[col].astype(np.int32)
            else:
                df[col] = df[col].astype(np.int64)      
        elif df[col].dtype == float:
            df[col] = df[col].astype(np.float32)
        else:
            continue
    print("___MEMORY USAGE AFTER COMPLETION:___")
    mem_usg = df.memory_usage().sum() / 1024**2 
    print("Memory usage is: ",mem_usg," MB")
    print("This is ",100*mem_usg/start_mem_usg,"% of the initial size")

reduce_mem(df_train)
print("---"*10)
reduce_mem(df_test)

"""#### Observer data 

"""

ds.structdata.check_train_test_set(df_train, df_test, index=None, col=None)

ds.structdata.describe(df_train)

ds.structdata.describe(df_test)

"""### Data Cleaning

#### Filling missing value
"""

# fill with most frequent
df_all['flbmk'] = df_all['flbmk'].fillna('N')
df_all['flg_3dsmk'] = df_all['flg_3dsmk'].fillna('N')

"""#### Handle different data type encoding

"""

cat_feats = ds.structdata.get_cat_feats(df_all)
num_feats = ds.structdata.get_num_feats(df_all)
get_unique_counts = ds.structdata.get_unique_counts(df_all)
all_feats = df_all.keys()

get_unique_counts

num_feats

cat_feats

all_feats

# label encoding catogorical feature
le = LabelEncoder()
for feat in cat_feats:
    df_all[feat] = le.fit_transform(df_all[feat].astype(str))

# one hot feature

# 'contp': 交易類別  
# 'flbmk': fallback交易註記 
# 'ecfg': 網路交易註記  
# 'flg_3dsmk': 3DS交易驗證註記
# 'hcefg': 支付型態
# 'insfg': 分期交易註記
# 'ovrlt': 超額交易註記
# 'stscd': 狀態碼
# 'iterm': 分期期數
onehot_feature = [
    'contp', 'flbmk', 'ecfg', 'flg_3dsmk', 'hcefg', 'insfg', 'ovrlt', 
    'stscd', 'iterm'
]

# frequency feature

# 'csmcu': 消費地幣別
# 'etymd': 交易型態（網路/實體）
# 'mcc': 特店類別
# 'mchno': 特店名稱
# 'acqic': 收單行代碼
# 'bacno': 歸戶帳號
# 'cano': 卡號
# 'scity': 消費地城市
# 'stocn': 消費地國別
freq_feature = [
    'csmcu', 'etymd', 'mcc', 'mchno', 'acqic', 'bacno', 'cano', 'scity',
    'stocn'
]

for k in freq_feature:
    df_all[k + '_f'] = df_all[k].map(df_all[k].value_counts(normalize=True))
    
for k in onehot_feature:
    add_dumy = pd.get_dummies(df_all[k])
    add_dumy.columns = [k + "_{}".format(x) for x in add_dumy.columns]
    if add_dumy.shape[0] < 2:
        add_dumy = add_dumy.iloc[:, 0]
    df_all = pd.concat([df_all, add_dumy], axis=1)

# filter given feature correlated to target feature, use threshold to set it to 0 and 1 (1 means high chance of fraud)
def identify_high_risk_fraud(df, feat, threshold):
    Top_array = df[f'{feat}'].value_counts().values[:threshold]
    df[f'{feat}_high_risk_fraud'] = df[f'{feat}'].apply(lambda x : 0 if x not in Top_array else x)

for feat in freq_feature:
    identify_high_risk_fraud(df_all, feat , 15)

"""#### Individual feature prepocess

##### acqic 收單行代碼
"""

Df = df_all.groupby(['acqic'])['fraud_ind'].agg(['mean', 'count']).reset_index().sort_values('count', ascending = False)
Df

"""##### bacno 歸戶帳號

"""

Df = df_all.groupby(['bacno'])['fraud_ind'].agg(['mean', 'count']).reset_index().sort_values('count', ascending = False)
Df

"""##### locdf & loctm 授權日期與時間

"""

time2val = lambda x: np.sin((x/12-1)*np.pi)
def fn(x): 
    x_str = str(int(x)).zfill(6)
    h, m, s = float(x_str[:2]), float(x_str[2:4]), float(x_str[4:])
    v = h + m/60 + s/3600
    return time2val(v)

# Sort by time, because transactions has time dependency
df_all['loctm']=  df_all['loctm'].apply(fn)

df_all=df_all.sort_values(by=['locdt','loctm'])

df_all['loctm'].describe()

def Str_turn_time(str1):
    str1 = str(int(str1))
    if len(str1) < 6:
        str1 = (6 - len(str1)) * '0' + str1
    return str1

df_all['Hour'] = df_all['loctm'].apply(lambda x :Str_turn_time(x)[:2]).astype(int)
df_all['Morning'] = 0
df_all.loc[(df_all['Hour'].astype('int') > 7) & (df_all['Hour'].astype('int') < 22), 'Morning'] = 1

df_all.loc[df_all['locdt'] < 121, 'Month'] = 4
df_all.loc[(df_all['locdt']) < 91, 'Month']  = 3
df_all.loc[(df_all['locdt']) < 61, 'Month']  = 2
df_all.loc[(df_all['locdt']) < 31, 'Month'] = 1

df_all['Week'] = df_all['locdt'].apply(lambda x : x%7)

identify_high_risk_fraud(df_all, 'Month' , 15)
identify_high_risk_fraud(df_all, 'Week' , 15)

"""##### conam 交易金額


"""

Df = df_all.groupby(['conam'])['fraud_ind'].agg(['mean']).reset_index().sort_values('mean', ascending = True)
Df

"""##### txkey 交易唯一序號


"""

df_all['Count_txkey_gb_bacno'] =  df_all.groupby(['bacno'])['txkey'].transform('count')

#Calculate count of transaction in the same account and in the same card
df_all['Count_txkey_gb_bacno'] =  df_all.groupby(['bacno', 'cano'])['txkey'].transform('count')

#Calculate count of transaction in the same account in the same one hour
df_all['Count_txkey_gb_bacno_locdt_Hour'] =  df_all.groupby(['bacno', 'locdt', 'Hour'])['txkey'].transform('count')

"""##### scity & stocn 消費城市&國別



"""

new_col = 'scity'+'_'+'stocn'
col = 'scity'
col2 = 'stocn'

df_all[new_col] = df_all[col].astype(str)+'_'+df_all[col2].astype(str)
df_all[new_col] = df_all[col].astype(str)+'_'+df_all[col2].astype(str) 

le = LabelEncoder()
df_all[new_col] = le.fit_transform(df_all[new_col].astype(str))

df_all.keys()

"""#### numerical feature -> StandardScaler
After processing all the feature, scale the numerical feature, so that the model can fit the training data better.
"""

num_feats = ['acqic','bacno','cano','conam', 'csmcu','hcefg','iterm', 'locdt','mcc',
 'mchno','scity','stocn', 'stscd','txkey','Month','Week']

scaler = StandardScaler()
df_all[num_feats] = scaler.fit_transform(df_all[num_feats])

# train and test data after data prepocessing
df_train = df_all.iloc[:len(df_train)]
df_test = df_all.iloc[len(df_train):]

# Time seires split

# X = df_train.drop('fraud_ind', axis=1)
# y = df_train['fraud_ind']

# transaction data has time dependency, so instead of normal train test split, here we use TimeSeriesSplit
# tscv = TimeSeriesSplit(n_splits=5)

# We have sorted the data by time(locdt, locdm) during data preprocess
# Time dependent data, train set is the first 70% of the original data
# X_train, X_test = np.split(X, [int(.7 *len(X))])
# y_train, y_test = np.split(y, [int(.7 *len(y))])

# # For training
# # X_train, X_test, y_train, y_test = TimeSeriesSplit(X, y, test_size=0.25, random_state=0, stratify=y)

# X_train = X_train.values
# X_test = X_test.values
# y_train = y_train.values
# y_test = y_test.values

# # For prediction
# X_predict = df_test.drop('fraud_ind', axis=1).values
# y_answer = df_test['fraud_ind'].values

"""### Feature selection



"""

X = df_train.drop('fraud_ind', axis=1)
y = df_train['fraud_ind']

# Since this data set is imbalance, use under sampling, for better training result of models 
X_nearmiss, y_nearmiss = NearMiss(n_jobs=-1).fit_resample(X, y)

print(X.shape)
print(y.shape)

print(X_nearmiss.shape)
print(y_nearmiss.shape)

from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import SelectFromModel

clf = ExtraTreesClassifier(n_estimators=500)
clf = clf.fit(X_nearmiss, y_nearmiss)

# Tree base prune
sel = SelectFromModel(clf, prefit=True, threshold=0.01)
tree_pruned_features_1 = [f for f, s in zip(X, sel.get_support()) if s]

print('\n The selected features are {}:'.format(tree_pruned_features_1))
print('\n threshold: 0.01')
print('\n The number of selected feature {}:'.format(len(tree_pruned_features_1)))

# Tree base prune
sel = SelectFromModel(clf, prefit=True, threshold=0.02)
tree_pruned_features_2 = [f for f, s in zip(X, sel.get_support()) if s]

print('\n The selected features are {}:'.format(tree_pruned_features_2))
print('\n threshold: 0.02')
print('\n The number of selected feature {}:'.format(len(tree_pruned_features_2)))

# Tree base prune
sel = SelectFromModel(clf, prefit=True, threshold=0.03)
tree_pruned_features_3 = [f for f, s in zip(X, sel.get_support()) if s]

print('\n The selected features are {}:'.format(tree_pruned_features_3))
print('\n threshold: 0.03')
print('\n The number of selected feature {}:'.format(len(tree_pruned_features_3)))

temp_df_train = pd.concat([df_train[tree_pruned_features_1], df_train['fraud_ind']], axis=1)
temp_df_test = pd.concat([df_test[tree_pruned_features_1], df_test['fraud_ind']], axis=1)

temp_df_train.to_csv('./dataAfterPruned/tree1_train.csv', index=False)
temp_df_test.to_csv('./dataAfterPruned/tree1_test.csv', index=False)

temp_df_train = pd.concat([df_train[tree_pruned_features_2], df_train['fraud_ind']], axis=1)
temp_df_test = pd.concat([df_test[tree_pruned_features_2], df_test['fraud_ind']], axis=1)

temp_df_train.to_csv('./dataAfterPruned/tree2_train.csv', index=False)
temp_df_test.to_csv('./dataAfterPruned/tree2_test.csv', index=False)

temp_df_train = pd.concat([df_train[tree_pruned_features_3], df_train['fraud_ind']], axis=1)
temp_df_test = pd.concat([df_test[tree_pruned_features_3], df_test['fraud_ind']], axis=1)

temp_df_train.to_csv('./dataAfterPruned/tree3_train.csv', index=False)
temp_df_test.to_csv('./dataAfterPruned/tree3_test.csv', index=False)

# This block takes too long to executed.
# To get the dataFrame after feature selection, please use gdown to download the csv file in the next block.

# Feature ranking with recursive feature elimination and cross-validated selection of the best number of features.
skf = StratifiedKFold(n_splits=5, random_state=None, shuffle=True)
xgb = XGBClassifier(n_estimators=600, n_jobs=-1, objective='binary:hinge')
rfecv = RFECV(estimator=xgb, step=1, cv=skf, verbose=3, n_jobs = -1, scoring = 'f1')

rfecv.fit(X_nearmiss, y_nearmiss)

RFECV_pruned_features = [f for f, s in zip(X, rfecv.support_) if s]

print('\n The selected features are {}:'.format(RFECV_pruned_features))
print('\n The number of selected feature {}:'.format(len(RFECV_pruned_features)))

temp_df_train = pd.concat([df_train[RFECV_pruned_features], df_train['fraud_ind']], axis=1)
temp_df_test = pd.concat([df_test[RFECV_pruned_features], df_test['fraud_ind']], axis=1)

temp_df_train.to_csv('./dataAfterPruned/RFECV_train.csv', index=False)
temp_df_test.to_csv('./dataAfterPruned/RFECV_test.csv', index=False)

df_train_RFECV = pd.read_csv('./dataAfterPruned/RFECV_train.csv')
df_test_RFECV = pd.read_csv('./dataAfterPruned/RFECV_test.csv')

df_train_tree1 = pd.read_csv('./dataAfterPruned/tree1_train.csv')
df_test_tree1 = pd.read_csv('./dataAfterPruned/tree1_test.csv')

df_train_tree2 = pd.read_csv('./dataAfterPruned/tree2_train.csv')
df_test_tree2 = pd.read_csv('./dataAfterPruned/tree2_test.csv')

df_train_tree3 = pd.read_csv('./dataAfterPruned/tree3_train.csv')
df_test_tree3 = pd.read_csv('./dataAfterPruned/tree3_test.csv')

reduce_mem(df_train)
reduce_mem(df_test)

"""#### Compare datasets performance on basic classification model

"""

def evaluateDataset(dataset, model, name):
    
    X = dataset.drop('fraud_ind', axis=1).values
    y = dataset['fraud_ind'].values
    
    accuracy_lst = []
    precision_lst = []
    recall_lst = []
    f1_lst = []
    f1_micro_lst = []
    f1_macro_lst = []
    f1_weighted_lst = []
    auc_lst = []
    
    
    skf = StratifiedKFold(n_splits=5, random_state=None, shuffle=True)

    for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):
        X_train = X[train_index]
        y_train = y[train_index] 
        X_test = X[test_index]
        y_test = y[test_index] 

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        accuracy_lst.append(model.score(X_test, y_test))
        precision_lst.append(precision_score(y[test_index], y_pred))
        recall_lst.append(recall_score(y[test_index], y_pred))
        f1_lst.append(f1_score(y_test, y_pred))
        f1_micro_lst.append(f1_score(y_test, y_pred, average='micro'))
        f1_macro_lst.append(f1_score(y_test, y_pred, average='macro'))
        f1_weighted_lst.append(f1_score(y_test, y_pred, average='weighted'))
        auc_lst.append(roc_auc_score(y[test_index], y_pred))
        
    print(f'Dataset: {name}')
    print('---' * 45)
    print("accuracy: {}".format(np.mean(accuracy_lst)))
    print("precision: {}".format(np.mean(precision_lst)))
    print("recall: {}".format(np.mean(recall_lst)))
    print("f1: {}".format(np.mean(f1_lst)))
    print("f1 micro: {}".format(np.mean(f1_micro_lst)))
    print("f1 macro : {}".format(np.mean(f1_macro_lst)))
    print("f1 weighted: {}".format(np.mean(f1_weighted_lst)))
    print('---' * 45)

xgb = XGBClassifier(objective='binary:hinge', n_jobs=-1, verbosity=0)

dataSets = [df_train_RFECV,df_train_tree1,df_train_tree2, df_train_tree3]
name = ['RFECV', 'tree1', 'tree2', 'tree3']

for dataset, name in zip(dataSets,name):
    evaluateDataset(dataset, xgb, name)

# tree 2 has the highest f1-score: 0.6979911044445455
# use tree 2 dataset to para tuning

"""## Parameter tuning

This data set is highly imbalance, in order to fit the training data better, under sampling the data first, use it to train the model, and get the best hyperparameter, then use the hyperparameter obtained to fit the test set and make prediction.
"""

# df_train = pd.read_csv('./dataAfterPruned/tree2_train.csv')
# df_test = pd.read_csv('./dataAfterPruned/tree2_test.csv')

# Inorder to start right here, down below is the gdown link to the dataset - tree2, the one we just clean up and went through feature selection.
!pip3 install gdown
import gdown

f1id = '1PGcWwSgWJKc6CJXU3R9y3_AsYObrUT0C'
f2id = '1GiuDe0BTaq8yISEJgWdnTnHhTP_v_1bM'

url = 'https://drive.google.com/uc?id=%s'%(f1id)
output = 'train.csv'
gdown.download(url, output, quiet=False)
url = 'https://drive.google.com/uc?id=%s'%(f2id)
output = 'test.csv'
gdown.download(url, output, quiet=False)

df_train = pd.read_csv('./train.csv')
df_test = pd.read_csv('./test.csv')

X = df_train.drop('fraud_ind', axis=1)
y = df_train['fraud_ind']
# Since this data set is imbalance, use under sampling, for better training result of models 
X_nearmiss, y_nearmiss = NearMiss(n_jobs=-1).fit_resample(X, y)

print(X_nearmiss.shape)
print(y_nearmiss.shape)
print(X.shape)
print(y.shape)

"""### hyperOpt


"""

from hyperopt import hp, fmin, tpe, STATUS_OK, Trials

space = {'learning_rate': hp.uniform('learning_rate', 0.001, 0.5),
        'max_depth': hp.choice('max_depth', [1, 3, 5, 9]),
        'colsample_bytree': hp.choice('colsample_bytree', [0.1, 0.7, 1.0]),
        'subsample': hp.choice ('subsample', [0.1, 0.5, 1.0]),
        'min_child_weight' : hp.choice ('min_child_weight', [0.1, 0.5, 1.0]),
        'n_estimators' : hp.choice('n_estimators', [600, 1600, 2100])
    }

def objective(space):
    model = XGBClassifier(learning_rate = space['learning_rate'], 
                                   max_depth = space['max_depth'],
                                 colsample_bytree = space['colsample_bytree'],
                                 subsample = space['subsample'],
                                 n_estimators = space['n_estimators'],
                                min_child_weight=space['min_child_weight'],
                              objective='binary:hinge'
                                 )
    
    accuracy = cross_val_score(model, X_nearmiss, y_nearmiss, cv = 3, scoring='f1', n_jobs=-1).mean()

    # We aim to maximize accuracy, therefore we return it as a negative value
    return {'loss': -accuracy, 'status': STATUS_OK }
    
best = fmin(fn= objective,
            space= space,
            algo= tpe.suggest,
            max_evals = 50,
            trials= Trials())

best

# 20 iters

# {'colsample_bytree': 3,
#  'learning_rate': 0.05315556869068875,
#  'max_depth': 2,
#  'min_child_weight': 0,
#  'n_estimators': 0,
#  'subsample': 3}

# 30 iters
# {'colsample_bytree': 0,
#  'learning_rate': 0.05437163768633186,
#  'max_depth': 3,
#  'min_child_weight': 2,
#  'n_estimators': 0,
#  'subsample': 1}

# 50iters
# {'colsample_bytree': 0,
#  'learning_rate': 0.1282598147882257,
#  'max_depth': 2,
#  'min_child_weight': 1,
#  'n_estimators': 1,
#  'subsample': 2}

hyperOPT_xgb_model = XGBClassifier(
    objective='binary:hinge', 
    n_jobs=-1, 
    colsample_bytree= 1,
  learning_rate= 0.05315556869068875,
  max_depth= 9,
  min_child_weight= 0.2,
  n_estimators= 600,
  subsample= 1)

basicTrainAndTest(df_train, df_test, hyperOPT_xgb_model, "hyperOPT_xgb_model")

# for 20 iters
# Before

# Dataset: tree2
# ---------------------------------------------------------------------------------------------------------------------------------------
# accuracy: 0.9932088597613331
# precision: 0.8627229751222109
# recall: 0.5861425883775457
# f1: 0.6979911044445455
# f1 micro: 0.9932088597613331
# f1 macro : 0.8472784606458198
# f1 weighted: 0.9925677730156321
    
# After

# Dataset: hyperOPT_xgb_model
# ---------------------------------------------------------------------------------------------------------------------------------------
# Accuracy: 0.9943724092974843
# Precision: 0.8843430369787569
# Reacall: 0.6648264984227129
# f1-score: 0.7590320765334835
# micro f1-score: 0.9943724092974843
# macro f1-score: 0.87809251806951
# weighted f1-score: 0.9939784067271029
# AUC: 0.8318258366239439

# f1 score improves from 0.6979911044445455 -> 0.7590320765334835 with 20 iter of tuning

hyperOPT_xgb_model_30iters = XGBClassifier(
    objective='binary:hinge', 
    n_jobs=-1, 
    colsample_bytree= 0.1,
 learning_rate= 0.05437163768633186,
 max_depth= 9,
 min_child_weight= 1.0,
 n_estimators= 600,
 subsample= 0.5)

basicTrainAndTest(df_train, df_test, hyperOPT_xgb_model_30iters, "hyperOPT_xgb_model_30iters")
# Before

# Dataset: tree2
# ---------------------------------------------------------------------------------------------------------------------------------------
# Accuracy: 0.9934051260753798
# Precision: 0.8685648547598505
# Reacall: 0.5954258675078864
# f1-score: 0.6979911044445455
# micro f1-score: 0.9934051260753798
# macro f1-score: 0.8515902378613556
# weighted f1-score: 0.992796908911799
# AUC: 0.7971042091452186

# After 

# Dataset: hyperOPT_xgb_model_30iters
# ---------------------------------------------------------------------------------------------------------------------------------------
# Accuracy: 0.9931817046789697
# Precision: 0.8963531669865643
# Reacall: 0.5524447949526814
# f1-score: 0.683581361307636
# micro f1-score: 0.9931817046789697
# macro f1-score: 0.8400675414316088
# weighted f1-score: 0.9923812722757145
# AUC: 0.7757908290447723
# ---------------------------------------------------------------------------------------------------------------------------------------

# f1 score improves from 0.6979911044445455 -> 0.683581361307636 with 30 iter of tuning
# --> overFitting ?

hyperOPT_xgb_model_50iters = XGBClassifier(
    objective='binary:hinge', 
    n_jobs=-1, 
    colsample_bytree= 0.1,
 learning_rate= 0.1282598147882257,
 max_depth= 5,
 min_child_weight= 0.5,
 n_estimators= 600,
 subsample= 1)

basicTrainAndTest(df_train, df_test, hyperOPT_xgb_model_50iters, "hyperOPT_xgb_model_50iters")

# for 50 iters
# Before

# Dataset: tree2
# ---------------------------------------------------------------------------------------------------------------------------------------
# accuracy: 0.9932088597613331
# precision: 0.8627229751222109
# recall: 0.5861425883775457
# f1: 0.6979911044445455
# f1 micro: 0.9932088597613331
# f1 macro : 0.8472784606458198
# f1 weighted: 0.9925677730156321
    
# After

# Accuracy: 0.9925981805612871
# Precision: 0.8857729138166894
# Reacall: 0.5106466876971609
# f1-score: 0.647823911955978
# micro f1-score: 0.9925981805612871
# macro f1-score: 0.8220418487390271
# weighted f1-score: 0.9916145478129411
# AUC: 0.754878455403692

# f1 score improves from 0.6979911044445455 -> 0.647823911955978 with 50 iter of tuning
# ---> Overfitting ???

"""### optuna"""

# ! pip install optuna
import optuna
from optuna import Trial, visualization
from optuna.samplers import TPESampler

def objective(trial, X, y):

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)
    
    X_train = X_train.values
    X_test = X_test.values
    y_train = y_train.values
    y_test = y_test.values
    
    param = {
        "verbosity": 0,
        "objective": "binary:hinge",
        # defines booster, gblinear for linear functions.
        "booster": trial.suggest_categorical("booster", ["gbtree", "gblinear", "dart"]),
        # L2 regularization weight.
        "lambda": trial.suggest_float("lambda", 1e-8, 1.0, log=True),
        # L1 regularization weight.
        "alpha": trial.suggest_float("alpha", 1e-8, 1.0, log=True),
        # sampling ratio for training data.
        "subsample": trial.suggest_float("subsample", 0.2, 0.7,log=True),
        # sampling according to each tree.
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
        "n_estimators":trial.suggest_int("n_estimators", 600, 2000),
        'learning_rate':trial.suggest_loguniform('learning_rate',0.005,0.5),
        "n_jobs": -1
    }
    
    model = XGBClassifier(**param)

    if param["booster"] in ["gbtree", "dart"]:
        # maximum depth of the tree, signifies complexity of the tree.
        param["max_depth"] = trial.suggest_int("max_depth", 3, 9, step=2)
        # minimum child weight, larger the term more conservative the tree.
        param["min_child_weight"] = trial.suggest_int("min_child_weight", 2, 10)
        # defines how selective algorithm is.
        param["gamma"] = trial.suggest_float("gamma", 1e-8, 1.0, log=True)
        param["grow_policy"] = trial.suggest_categorical("grow_policy", ["depthwise", "lossguide"])

    if param["booster"] == "dart":
        param["sample_type"] = trial.suggest_categorical("sample_type", ["uniform", "weighted"])
        param["normalize_type"] = trial.suggest_categorical("normalize_type", ["tree", "forest"])
        param["rate_drop"] = trial.suggest_float("rate_drop", 1e-8, 1.0, log=True)
        param["skip_drop"] = trial.suggest_float("skip_drop", 1e-8, 1.0, log=True)

    model.fit(X_train, y_train)
    f1 = cross_val_score(model, X_test, y_test, scoring="f1").mean()
    return f1

study = optuna.create_study(direction='maximize',sampler=TPESampler())
study.optimize(lambda trial : objective(trial,X_nearmiss,y_nearmiss),n_trials= 15)

print("Number of finished trials: ", len(study.trials))
print("Best trial:")
trial = study.best_trial

print('Best trial: score {},\nparams {}'.format(study.best_trial.value,study.best_trial.params))
print("  Value: {}".format(trial.value))
print("  Params: ")
for key, value in trial.params.items():
    print("    {}: {}".format(key, value))

Optuna_model_15iters = XGBClassifier(
    objective='binary:hinge', 
    n_jobs=4, 
    booster= 'dart', 
    reg_lambda= 1.313584992794318e-08,
    alpha= 0.03520268400768686, 
    subsample= 0.5451488283528434,
    colsample_bytree= 0.9794897257170234,
    n_estimators= 1687, 
    learning_rate= 0.01364832787281915, 
    max_depth= 3, 
    min_child_weight= 10, 
    gamma= 0.41976538712494593, 
    grow_policy= 'lossguide', 
    sample_type= 'uniform', 
    normalize_type= 'forest', 
    rate_drop= 0.49776567113417985, 
    skip_drop= 0.5573682745996533)

basicTrainAndTest(df_train, df_test, Optuna_model_15iters, "Optuna_model_15iters")

# f1 score improves from 0.6979911044445455 -> 0.74937609384564835 with 15 iter of tuning

study

import joblib
joblib.dump(study, './study.pkl')

study = joblib.load('./study.pkl')
study.trials # error

"""### tpot

"""

X_test = df_test.drop('fraud_ind', axis=1).values
y_test = df_test['fraud_ind'].values

# !pip install tpot

# generic optimization
from tpot import TPOTClassifier

pipeline_optimizer = TPOTClassifier(generations=8, population_size=100, cv=5,
                                    random_state=42, verbosity=2, early_stop= 12,
                                    scoring = 'f1',  n_jobs=-2)

pipeline_optimizer.fit(X_nearmiss, y_nearmiss)
pipeline_optimizer.score(X_test, y_test)
pipeline_optimizer.export('./tpot_exported_pipeline.py')

from sklearn.ensemble import ExtraTreesClassifier
TpotClf = ExtraTreesClassifier(bootstrap=False, criterion="entropy", max_features=0.9000000000000001, min_samples_leaf=1, min_samples_split=2, n_estimators=100)

basicTrainAndTest(df_train, df_test, TpotClf, "TpotClf")

Dataset: TpotClf
# ---------------------------------------------------------------------------------------------------------------------------------------
# Accuracy: 0.995644597013513
# Precision: 0.9093742507791897
# Reacall: 0.7478312302839116
# f1-score: 0.8207292004760359
# micro f1-score: 0.995644597013513
# macro f1-score: 0.9092623599695955
# weighted f1-score: 0.9954349268710131
# AUC: 0.8734121186384594
# ---------------------------------------------------------------------------------------------------------------------------------------

pipeline_optimizer2 = TPOTClassifier(generations=10, population_size=200, cv=5,
                                    random_state=42, verbosity=2, early_stop= 12,
                                    scoring = 'f1',  n_jobs=-2)

pipeline_optimizer2.fit(X_nearmiss, y_nearmiss)
pipeline_optimizer2.score(X_test, y_test)
pipeline_optimizer2.export('./tpot_exported_pipeline2.py')

"""## Predict & Evaluation


"""

bestmodel = TpotClf

X = df_train.drop('fraud_ind', axis=1).values
y = df_train['fraud_ind'].values

# For prediction
X_test = df_test.drop('fraud_ind', axis=1).values
y_test = df_test['fraud_ind'].values

model.fit(X, y)
y_pred = model.predict(X_test)
print('---' * 45)
print(f'Accuracy: {model.score(X_test, y_test)}')
print(f'Precision: {precision_score(y_test, y_pred)}')
print(f'Reacall: {recall_score(y_test, y_pred)}')
print(f'f1-score: {f1_score(y_test, y_pred)}')
print(f"micro f1-score: {f1_score(y_test, y_pred, average='micro')}")
print(f"macro f1-score: {f1_score(y_test, y_pred, average='macro')}")
print(f"weighted f1-score: {f1_score(y_test, y_pred, average='weighted')}")
print(f'AUC: {roc_auc_score(y_test, y_pred)}')
print('---' * 45)