{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "finnish-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Performance Evaluation tools\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('./dataAfterPruned/tree2_train.csv')\n",
    "df_test = pd.read_csv('./dataAfterPruned/tree2_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alternate-connection",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30566, 23)\n",
      "(30566,)\n",
      "(1141340, 23)\n",
      "(1141340,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "\n",
    "X = df_train.drop('fraud_ind', axis=1)\n",
    "y = df_train['fraud_ind']\n",
    "# Since this data set is imbalance, use under sampling, for better training result of models \n",
    "X_nearmiss, y_nearmiss = NearMiss(n_jobs=-1).fit_resample(X, y)\n",
    "\n",
    "print(X_nearmiss.shape)\n",
    "print(y_nearmiss.shape)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accepted-absolute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Optimization Progress'), FloatProgress(value=0.0, max=2200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.9062494560845027\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.9062494560845027\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.9062494560845027\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.9062494560845027\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.9062494560845027\n",
      "\n",
      "Generation 6 - Current best internal CV score: 0.9062494560845027\n",
      "\n",
      "Generation 7 - Current best internal CV score: 0.9063186862113612\n",
      "\n",
      "Generation 8 - Current best internal CV score: 0.9063186862113612\n",
      "\n",
      "Generation 9 - Current best internal CV score: 0.9070076889960192\n",
      "\n",
      "Generation 10 - Current best internal CV score: 0.9070076889960192\n",
      "\n",
      "Best pipeline: GradientBoostingClassifier(MaxAbsScaler(input_matrix), learning_rate=0.5, max_depth=10, max_features=1.0, min_samples_leaf=13, min_samples_split=4, n_estimators=100, subsample=1.0)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-da31faca3980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpipeline_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_nearmiss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_nearmiss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpipeline_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mpipeline_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./tpot_exported_pipeline2.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "\n",
    "pipeline_optimizer = TPOTClassifier(generations=10, population_size=200, cv=5,\n",
    "                                    random_state=42, verbosity=2, early_stop= 12,\n",
    "                                    scoring = 'f1',  n_jobs=-2)\n",
    "\n",
    "pipeline_optimizer.fit(X_nearmiss, y_nearmiss)\n",
    "# pipeline_optimizer.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# TPOT will evaluate population_size + generations Ã— offspring_size pipelines in total.\n",
    "# 100 + 8*100 = 900 for 3 hours\n",
    "# 200 + 10*100 = 1200 for ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e05f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer.export('./tpot_exported_pipeline2.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a425557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(learning_rate=0.5, max_depth=10, max_features=1.0, min_samples_leaf=13, min_samples_split=4, n_estimators=100, subsample=1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "classified-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basicTrainAndTest(train, test, model, name):\n",
    "    X = train.drop('fraud_ind', axis=1).values\n",
    "    y = train['fraud_ind'].values\n",
    "    \n",
    "    # For prediction\n",
    "    \n",
    "    X_test = test.drop('fraud_ind', axis=1).values\n",
    "    y_test = test['fraud_ind'].values\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f'Dataset: {name}')\n",
    "    print('---' * 45)\n",
    "    print(f'Accuracy: {model.score(X_test, y_test)}')\n",
    "    print(f'Precision: {precision_score(y_test, y_pred)}')\n",
    "    print(f'Reacall: {recall_score(y_test, y_pred)}')\n",
    "    print(f'f1-score: {f1_score(y_test, y_pred)}')\n",
    "    print(f\"micro f1-score: {f1_score(y_test, y_pred, average='micro')}\")\n",
    "    print(f\"macro f1-score: {f1_score(y_test, y_pred, average='macro')}\")\n",
    "    print(f\"weighted f1-score: {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(f'AUC: {roc_auc_score(y_test, y_pred)}')\n",
    "    print('---' * 45)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "prime-tomato",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: GradientBoostingClassifier\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "Accuracy: 0.989309943303535\n",
      "Precision: 0.6066199872692553\n",
      "Reacall: 0.5636829652996845\n",
      "f1-score: 0.5843638221768013\n",
      "micro f1-score: 0.989309943303535\n",
      "macro f1-score: 0.7894745809938337\n",
      "weighted f1-score: 0.9891163952865567\n",
      "AUC: 0.7793719521803119\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "basicTrainAndTest(df_train, df_test, clf, \"GradientBoostingClassifier\")\n",
    "\n",
    "# Dataset: GradientBoostingClassifier\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Accuracy: 0.989309943303535\n",
    "# Precision: 0.6066199872692553\n",
    "# Reacall: 0.5636829652996845\n",
    "# f1-score: 0.5843638221768013\n",
    "# micro f1-score: 0.989309943303535\n",
    "# macro f1-score: 0.7894745809938337\n",
    "# weighted f1-score: 0.9891163952865567\n",
    "# AUC: 0.7793719521803119\n",
    "# ---------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
